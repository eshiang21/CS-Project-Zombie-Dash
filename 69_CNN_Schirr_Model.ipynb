{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVB0NHaH6c+GgatySDK/Nt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshiang21/CS-Project-Zombie-Dash/blob/master/69_CNN_Schirr_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJT4eCQ-yH_s",
        "colab_type": "code",
        "outputId": "9990c58a-b685-4fe1-86ef-9ee4f2c8878c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne-j2XsSlpjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Schirr(nn.Module):\n",
        "    def __init__(self, input_size, num_electrodes=22, dropout_rate=0.5, filter_width=3, pool_width=3, \\\n",
        "                 c1=50, c2=50, c3=75, c4=75, c5=100, c6=100):\n",
        "        super(CNN_Schirr, self).__init__()\n",
        "        self.lr = 0.001\n",
        "        self.weight_decay = 0.005\n",
        "        self.name = \"CNN_Schirr\"\n",
        "\n",
        "        self.act = F.elu\n",
        "        self.pool = nn.MaxPool2d((1, pool_width))\n",
        "        self.drop = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.c1 = nn.Conv2d(1, c1, (1, filter_width))\n",
        "        self.bn1 = nn.BatchNorm2d(c1)\n",
        "        self.c2 = nn.Conv2d(c1, c1, (num_electrodes, 1))\n",
        "        self.bn2 = nn.BatchNorm2d(c1)\n",
        "        self.c3 = nn.Conv2d(c1, c2, (1, filter_width))\n",
        "        self.bn3 = nn.BatchNorm2d(c2)\n",
        "        self.c4 = nn.Conv2d(c2, c3, (1, filter_width))\n",
        "        self.bn4 = nn.BatchNorm2d(c3)\n",
        "        self.c5 = nn.Conv2d(c3, c4, (1, filter_width))\n",
        "        self.bn5 = nn.BatchNorm2d(c4)\n",
        "\n",
        "        self.c6 = nn.Conv2d(c4, c5, (1, filter_width))\n",
        "        self.bn6 = nn.BatchNorm2d(c5)\n",
        "        \n",
        "\n",
        "        self.out_size = input_size\n",
        "        for _ in range(5):\n",
        "            self.out_size = (self.out_size - filter_width + 1) // pool_width\n",
        "        self.out_size *= c5\n",
        "        self.lf = nn.Linear(self.out_size, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.c1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.c2(x)\n",
        "        x = self.act(self.bn2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.c3(x)\n",
        "        x = self.act(self.bn3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.c4(x)\n",
        "        x = self.act(self.bn4(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.c5(x)\n",
        "        x = self.act(self.bn5(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "        x = self.c6(x)\n",
        "        x = self.act(self.bn6(x))\n",
        "        x = self.pool(x)\n",
        "        \n",
        "\n",
        "        x = x.view(-1, self.out_size)\n",
        "        x = self.lf(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim=100, num_electrodes=22, num_layers=2, dropout_rate=0.8, bi=False, rnn_type=\"GRU\"):\n",
        "        super(RNN, self).__init__()\n",
        "        self.name = \"RNN\"\n",
        "        self.lr = 0.0005\n",
        "        self.weight_decay = 0.01\n",
        "\n",
        "        if rnn_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(num_electrodes, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate, bidirectional=bi)\n",
        "        elif rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(num_electrodes, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate, bidirectional=bi)\n",
        "        else:\n",
        "            print(\"Invalid rnn type. Must be 'GRU' or 'LSTM'. Got:\", rnn_type)\n",
        "            exit(1)\n",
        "            \n",
        "        self.out_size = input_size * hidden_dim\n",
        "        if bi:\n",
        "            self.out_size *= 2\n",
        "        self.drop = nn.Dropout(p=dropout_rate)\n",
        "        self.fc = nn.Linear(self.out_size, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.name = \"ConvNet\"\n",
        "        self.act = F.elu\n",
        "        self.c1 = nn.Conv2d(1, 8, (1, 129), padding=(0, 64))\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.c2 = nn.Conv2d(8, 16, (22, 1))\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.a2 = nn.AvgPool2d((1, 5))\n",
        "        # self.d2 = nn.Dropout(p=0.3)\n",
        "        self.c3 = nn.Conv2d(16, 16, (1, 65), groups=16, padding=(0, 32))\n",
        "        self.c4 = nn.Conv2d(16, 32, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.a4 = nn.AvgPool2d((1, 5))\n",
        "        # self.d4 = nn.Dropout(p=0.3)\n",
        "        self.c5 = nn.Conv2d(32, 32, (1, 33), groups=32, padding=(0, 16))\n",
        "        self.c6 = nn.Conv2d(32, 64, 1)\n",
        "        self.bn6 = nn.BatchNorm2d(64)\n",
        "        self.a6 = nn.AvgPool2d((1, 4))\n",
        "        self.out_size = 640\n",
        "        self.lf = nn.Linear(self.out_size, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # input should have length 1000\n",
        "        x = self.c1(x)\n",
        "        x = self.act(self.bn1(x))\n",
        "        x = self.c2(x)\n",
        "        x = self.act(self.bn2(x))\n",
        "        x = self.a2(x)\n",
        "        # x = self.d2(x)\n",
        "        x = self.c3(x)\n",
        "        x = self.c4(x)\n",
        "        x = self.act(self.bn4(x))\n",
        "        x = self.a4(x)\n",
        "        # x = self.d4(x)\n",
        "        x = self.c5(x)\n",
        "        x = self.c6(x)\n",
        "        x = self.act(self.bn6(x))\n",
        "        x = self.a6(x)\n",
        "        x = x.view(-1, self.out_size)\n",
        "        x = self.lf(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.name = \"ResNet\"\n",
        "        self.act = F.elu\n",
        "        self.c1 = nn.Conv2d(1, 8, (1, 129), padding=(0, 64))\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.c2 = nn.Conv2d(8, 16, (22, 1))\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.a2 = nn.AvgPool2d((1, 2))\n",
        "        self.c3 = nn.Conv2d(16, 16, (1, 65), groups=16, padding=(0, 32))\n",
        "        self.c4 = nn.Conv2d(16, 32, 1)\n",
        "        # self.c4r = nn.Conv2d(16, 32, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.a4 = nn.AvgPool2d((1, 2))\n",
        "        self.c5 = nn.Conv2d(32, 32, (1, 33), groups=32, padding=(0, 16))\n",
        "        self.c6 = nn.Conv2d(32, 64, 1)\n",
        "        # self.c6r = nn.Conv2d(32, 64, 1)\n",
        "        self.bn6 = nn.BatchNorm2d(64)\n",
        "        self.a6 = nn.AvgPool2d((1, 2))\n",
        "        self.c7 = nn.Conv2d(64, 64, (1, 17), groups=64, padding=(0, 8))\n",
        "        self.c8 = nn.Conv2d(64, 128, 1)\n",
        "        # self.c8r = nn.Conv2d(64, 128, 1)\n",
        "        self.bn8 = nn.BatchNorm2d(128)\n",
        "        self.a8 = nn.AvgPool2d((1,5))\n",
        "        self.c9 = nn.Conv2d(128, 128, (1, 9), groups=128, padding=(0, 4))\n",
        "        self.c10 = nn.Conv2d(128, 256, 1)\n",
        "        # self.c10r = nn.Conv2d(128, 256, 1)\n",
        "        self.bn10 = nn.BatchNorm2d(256)\n",
        "        self.a10 = nn.AvgPool2d((1,5))\n",
        "        self.out_size = 256 * 4\n",
        "        self.lf = nn.Linear(self.out_size, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # input should have length 800\n",
        "        x = self.c1(x)\n",
        "        x = self.act(self.bn1(x))\n",
        "        x = self.c2(x)\n",
        "        x = self.act(self.bn2(x))\n",
        "        y = self.a2(x)\n",
        "        x = self.c3(y)\n",
        "        x = self.c4(x)\n",
        "        x = self.act(self.bn4(x))\n",
        "        y = self.a4(x)\n",
        "        x = self.c5(y)\n",
        "        x = self.c6(x)\n",
        "        x = self.act(self.bn6(x))\n",
        "        y = self.a6(x)\n",
        "        x = self.c7(y)\n",
        "        x = self.c8(x)\n",
        "        x = self.act(self.bn8(x))\n",
        "        y = self.a8(x)\n",
        "        x = self.c9(y)\n",
        "        x = self.c10(x)\n",
        "        x = self.act(self.bn10(x))\n",
        "        x = self.a10(x)\n",
        "        x = x.view(-1, self.out_size)\n",
        "        x = self.lf(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class ResNetv2(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.name = \"ResNetv2\"\n",
        "        super(ResNetv2, self).__init__()\n",
        "        self.act = F.elu\n",
        "\n",
        "        self.c1 = nn.Conv2d(1, 8, (1, 129), padding=(0, 64))\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "\n",
        "        self.c2 = nn.Conv2d(8, 16, (22, 1))\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.a2 = nn.AvgPool2d((1, 5))\n",
        "\n",
        "        self.c3r1 = nn.Conv2d(16, 16, (1, 65), groups=16, padding=(0, 32))\n",
        "        self.c3r2 = nn.Conv2d(16, 16, 1)\n",
        "        self.bn3r2 = nn.BatchNorm2d(16)\n",
        "        self.c3r3 = nn.Conv2d(16, 16, (1, 65), groups=16, padding=(0, 32))\n",
        "        self.c3r4 = nn.Conv2d(16, 16, 1)\n",
        "        self.bn3r4 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.c3 = nn.Conv2d(16, 16, (1, 65), groups=16, padding=(0, 32))\n",
        "        self.c4 = nn.Conv2d(16, 32, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.a4 = nn.AvgPool2d((1, 5))\n",
        "\n",
        "        self.c5r1 = nn.Conv2d(32, 32, (1, 33), groups=32, padding=(0, 16))\n",
        "        self.c5r2 = nn.Conv2d(32, 32, 1)\n",
        "        self.bn5r2 = nn.BatchNorm2d(32)\n",
        "        self.c5r3 = nn.Conv2d(32, 32, (1, 33), groups=32, padding=(0, 16))\n",
        "        self.c5r4 = nn.Conv2d(32, 32, 1)\n",
        "        self.bn5r4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.c5 = nn.Conv2d(32, 32, (1, 33), groups=32, padding=(0, 16))\n",
        "        self.c6 = nn.Conv2d(32, 64, 1)\n",
        "        self.bn6 = nn.BatchNorm2d(64)\n",
        "        self.a6 = nn.AvgPool2d((1, 4))\n",
        "\n",
        "        self.out_size = 64 * 10\n",
        "        self.lf = nn.Linear(self.out_size, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # input should have length 1000\n",
        "        x = self.c1(x)\n",
        "        x = self.act(self.bn1(x))\n",
        "        x = self.c2(x)\n",
        "        x = self.act(self.bn2(x))\n",
        "        y = self.a2(x)\n",
        "\n",
        "        x = self.c3r1(y)\n",
        "        x = self.c3r2(x)\n",
        "        x = self.act(self.bn3r2(x))\n",
        "        x = self.c3r3(x)\n",
        "        x = self.c3r4(x)\n",
        "        x = self.act(self.bn3r4(x) + y)\n",
        "\n",
        "        x = self.c3(x)\n",
        "        x = self.c4(x)\n",
        "        x = self.act(self.bn4(x))\n",
        "        y = self.a4(x)\n",
        "\n",
        "        x = self.c5r1(y)\n",
        "        x = self.c5r2(x)\n",
        "        x = self.act(self.bn5r2(x))\n",
        "        x = self.c5r3(x)\n",
        "        x = self.c5r4(x)\n",
        "        x = self.act(self.bn5r4(x) + y)\n",
        "\n",
        "        x = self.c5(y)\n",
        "        x = self.c6(x)\n",
        "        x = self.act(self.bn6(x))\n",
        "        x = self.a6(x)\n",
        "\n",
        "        x = x.view(-1, self.out_size)\n",
        "        x = self.lf(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zURfOiLFlvRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def GetTrialAccuracy(votes):\n",
        "    acc = 0\n",
        "    for key in votes.keys():\n",
        "        maj_class = np.argmax(votes[key][:4])\n",
        "        if maj_class == votes[key][4]:\n",
        "            acc += 1\n",
        "\n",
        "    acc /= float(len(votes.keys()))\n",
        "    return 100. * acc\n",
        "\n",
        "def UpdateClassVotes(votes, preds, targets, trial_ids):\n",
        "    for pred_class, targ, trial_id in zip(preds, targets, trial_ids):\n",
        "        pred_class = pred_class.item()\n",
        "        trial_id = trial_id.item()\n",
        "        targ = targ.item()\n",
        "        if trial_id not in votes:\n",
        "            votes[trial_id] = np.zeros(5)\n",
        "        votes[trial_id][pred_class] += 1\n",
        "        votes[trial_id][4] = targ\n",
        "\n",
        "def UnpackDataLoader(item, device, use_gpu):\n",
        "    trial_ids = None\n",
        "    if len(item) == 3:\n",
        "            data, target, trial_ids = item\n",
        "    else:\n",
        "        data, target = item\n",
        "    if use_gpu:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "    return data, target, trial_ids\n",
        "\n",
        "def TrainNN(net, train_loader, optimizer, epoch, print_every=400, get_trial_acc=False, use_gpu=False, device=None):\n",
        "    criterion = nn.NLLLoss()\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    net.train()\n",
        "    votes = {}\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    print(\"Running epoch:\", epoch)\n",
        "    for idx, item in enumerate(train_loader):\n",
        "        total_idx = idx * train_loader.batch_size\n",
        "        if total_idx % print_every == 0 and total_idx != 0:\n",
        "            print(\"Iter {} - loss: {:.4f}, acc: {:.4f}\".format(total_idx, running_loss / print_every, running_acc / print_every))\n",
        "            running_loss = 0\n",
        "            running_acc = 0\n",
        "\n",
        "        data, target, trial_ids = UnpackDataLoader(item, device, use_gpu)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        net_out = net(data)\n",
        "        loss = criterion(net_out, target)\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(net.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss = loss.item() * len(data)\n",
        "        train_loss += total_loss\n",
        "        running_loss += total_loss\n",
        "\n",
        "        pred = net_out.argmax(dim=1, keepdim=True)\n",
        "        total_acc = pred.eq(target.view_as(pred)).sum().item()\n",
        "        running_acc += total_acc\n",
        "        correct += total_acc\n",
        "        if get_trial_acc:\n",
        "            UpdateClassVotes(votes, pred, target, trial_ids)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    acc = 100. * correct / len(train_loader.dataset)\n",
        "\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        train_loss, correct, len(train_loader.dataset), acc))\n",
        "    if get_trial_acc:\n",
        "        acc = GetTrialAccuracy(votes)\n",
        "        print('Train Trial Accuracy: {:.2f}%'.format(acc))\n",
        "    print()\n",
        "\n",
        "    return train_loss, acc\n",
        "\n",
        "def TestNN(net, test_loader, epoch, get_trial_acc=False, use_gpu=False, device=None):\n",
        "    net.eval()\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    votes = {}\n",
        "    with torch.no_grad():\n",
        "        for item in test_loader:\n",
        "            data, target, trial_ids = UnpackDataLoader(item, device, use_gpu)\n",
        "\n",
        "            output = net(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            \n",
        "            if get_trial_acc:\n",
        "                UpdateClassVotes(votes, pred, target, trial_ids)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), acc))\n",
        "\n",
        "    if get_trial_acc:\n",
        "        acc = GetTrialAccuracy(votes)\n",
        "        print('Test Trial Accuracy: {:.2f}%'.format(acc))\n",
        "    print()\n",
        "\n",
        "    return test_loss, acc\n",
        "\n",
        "def PlotNNResults(epochs, train_loss, train_acc, test_loss, test_acc, plot_type=\"\"):\n",
        "    fig, axs = plt.subplots(2, sharex=True)\n",
        "    axs[0].plot(epochs, train_acc, label=\"Train\")\n",
        "    axs[0].plot(epochs, test_acc, label=\"Test\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend()\n",
        "    axs[1].plot(epochs, train_loss, label=\"Train\")\n",
        "    axs[1].plot(epochs, test_loss, label=\"Test\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    fig.add_subplot(111, frameon=False)\n",
        "    plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
        "    plt.grid(False)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    title = \"Results\" if plot_type == \"\" else plot_type + \" Results\"\n",
        "    fig.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "def AddDir(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        os.makedirs(filename)\n",
        "\n",
        "def GetTrainingCommand(epochs):\n",
        "    end_training = False\n",
        "    while True:\n",
        "        command = input(\"Last epoch reached. Enter number of epochs to continue training for (0 if none): \")\n",
        "        if not command.isdigit():\n",
        "            print(\"Invalid option\")\n",
        "        elif int(command) == 0:\n",
        "            end_training = True\n",
        "            break\n",
        "        else:\n",
        "            epochs += list(range(1 + epochs[-1], 1 + epochs[-1] + int(command)))\n",
        "            break\n",
        "    return end_training, epochs\n",
        "\n",
        "def SaveTrainingResults(net, saved_model, model_epoch, saved_opt, saved_sched, cur_min_loss, train_loss, test_loss, train_acc, test_acc):\n",
        "    while True:\n",
        "        key = input(\"Save model and training history? (y/n): \")\n",
        "        if key is \"y\":\n",
        "            name = input(\"Enter your name: \")\n",
        "            extension = input(\"Enter unique model identifier (to be used as part of filename): \")\n",
        "            extension = net.name + \"_\" + extension\n",
        "            AddDir(\"training_results/\" + name)\n",
        "            np.savez(\"training_results/\" + name + \"/\" + extension + \".npz\", train_loss=train_loss, train_acc=train_acc, test_loss=test_loss, test_acc=test_acc)\n",
        "            \n",
        "            AddDir(\"saved_models/\" + name)\n",
        "            filename = \"saved_models/\" + name + \"/\" + extension + \".pt\"\n",
        "            torch.save({\n",
        "                'model' : saved_model,\n",
        "                'epoch' : model_epoch,\n",
        "                'optimizer' : saved_opt,\n",
        "                'scheduler' : saved_sched,\n",
        "                'loss' : cur_min_loss,\n",
        "            }, filename)\n",
        "\n",
        "            print(\"Saved {} model from epoch {} with test loss {}\".format(extension, model_epoch, cur_min_loss))\n",
        "            break\n",
        "        elif key is \"n\":\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid option\")\n",
        "\n",
        "def TrainTestModel(net, optimizer, train_loader, test_loader, num_epochs=20, use_gpu=True, get_trial_acc=False, print_every=None):\n",
        "    device = None\n",
        "    if use_gpu:\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda:0\")\n",
        "            net.to(device)\n",
        "        else:\n",
        "            print(\"GPU not available :(\")\n",
        "            exit(1)\n",
        "\n",
        "    epochs = list(range(num_epochs))\n",
        "    last_epoch = epochs[-1]\n",
        "    if print_every is None:\n",
        "        print_every = train_loader.batch_size * (len(train_loader.dataset) // (5 * train_loader.batch_size))\n",
        "\n",
        "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
        "    cur_min_loss = np.Inf\n",
        "    cur_best_acc = 0\n",
        "\n",
        "    break_count = 0\n",
        "\n",
        "    #change lr annealing#\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=10, verbose=True, threshold=0.02)\n",
        "    try:\n",
        "        idx = -1\n",
        "        while True:\n",
        "            idx += 1\n",
        "            epoch = epochs[idx]\n",
        "            train_l, train_a = TrainNN(net, train_loader, optimizer, epoch, print_every=print_every, get_trial_acc=get_trial_acc, use_gpu=use_gpu, device=device)\n",
        "            test_l, test_a = TestNN(net, test_loader, epoch, get_trial_acc=get_trial_acc, use_gpu=use_gpu, device=device)\n",
        "\n",
        "            if test_l < cur_min_loss:\n",
        "                cur_min_loss = test_l\n",
        "                saved_model = net.state_dict()\n",
        "                saved_opt = optimizer.state_dict()\n",
        "                saved_sched = scheduler.state_dict()\n",
        "                model_epoch = epoch\n",
        "                break_count = 0\n",
        "            elif test_l > cur_min_loss:\n",
        "                break_count += 1\n",
        "\n",
        "            if test_a > cur_best_acc:\n",
        "                cur_best_acc = test_a\n",
        "            \n",
        "            train_loss.append(train_l)\n",
        "            train_acc.append(train_a)\n",
        "            test_loss.append(test_l)\n",
        "            test_acc.append(test_a)\n",
        "\n",
        "            if break_count >= 15:\n",
        "                last_epoch = epoch\n",
        "                print(\"Stopping early due to stagnation of test loss\")\n",
        "                break\n",
        "\n",
        "            scheduler.step(test_l)\n",
        "\n",
        "            if epoch == epochs[-1]:\n",
        "                end_training, epochs = GetTrainingCommand(epochs)\n",
        "                if end_training:\n",
        "                    break\n",
        "                        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Quit training loop due to keyboard interrupt\")\n",
        "        \n",
        "    print(\"\\nBest test loss was {}\".format(cur_min_loss))\n",
        "    print(\"Best test accuracy was {}\\n\".format(cur_best_acc))\n",
        "    if not use_gpu:\n",
        "        PlotNNResults(range(last_epoch+1), train_loss, train_acc, test_loss, test_acc)\n",
        "\n",
        "    SaveTrainingResults(net, saved_model, model_epoch, saved_opt, saved_sched, cur_min_loss, train_loss, test_loss, train_acc, test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygxv82PXl_93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def AddNoiseData(data, labels, trials, noise_factor):\n",
        "    data_set = [data] \n",
        "    label_set = [labels]\n",
        "    trials *= (noise_factor + 1)\n",
        "    for _ in range(noise_factor):\n",
        "        noise = np.random.normal(0, 5.0, data.shape)\n",
        "        data_set.append(data + noise)\n",
        "        label_set.append(labels)\n",
        "\n",
        "    data_set = np.vstack(data_set)\n",
        "    label_set = np.hstack(label_set)\n",
        "\n",
        "    return data_set, label_set, trials\n",
        "\n",
        "def ApplySlidingWindow(data_set, label_set, trials, bin_size, start_width, slide_factor, random_choice=False):\n",
        "    data_res, label_res, trial_res = [], [], []\n",
        "    for i in range(len(data_set)):\n",
        "        if random_choice:\n",
        "            starts = np.random.choice(start_width, slide_factor, replace=False)\n",
        "        else:\n",
        "            starts = range(0, start_width+1, start_width//(slide_factor-1))\n",
        "        for start in starts:\n",
        "            data_res.append(data_set[i, :, start:start+bin_size])\n",
        "            label_res.append(label_set[i])\n",
        "            trial_res.append(trials[i])\n",
        "\n",
        "    return np.array(data_res), np.array(label_res), trial_res\n",
        "\n",
        "def SubsampleData(data, labels, trials, subsample_factor):\n",
        "    ret_data, ret_labels = [], []\n",
        "    trials *= subsample_factor\n",
        "    for i in range(subsample_factor):\n",
        "        ret_data.append(data[:, :, i::subsample_factor])\n",
        "        ret_labels.append(labels)\n",
        "    return np.vstack(ret_data), np.hstack(ret_labels), trials\n",
        "\n",
        "def AugmentData(data, labels, noise_factor=0, slide_factor=1, subsample_factor=1, bin_size=800, augment=False, train_data=True, model_type=\"conv\"):\n",
        "    trials = list(range(len(data)))\n",
        "    max_width = 1000 // subsample_factor\n",
        "    start_width = max_width - bin_size\n",
        "    if start_width < 0:\n",
        "        print(\"Invalid bin size and subsample configuration. Bin size must be <= 1000//subsample_factor. Given bin_size={}, subsample_factor={}.\".format(bin_size, subsample_factor))\n",
        "        exit(1)\n",
        "\n",
        "    if augment:\n",
        "        if subsample_factor > 1:\n",
        "            data, labels, trials = SubsampleData(data, labels, trials, subsample_factor)\n",
        "\n",
        "        if train_data and noise_factor > 0:\n",
        "            data, labels, trials = AddNoiseData(data, labels, trials, noise_factor)\n",
        "\n",
        "        if slide_factor > 1:\n",
        "            data, labels, trials = ApplySlidingWindow(data, labels, trials, bin_size, start_width, slide_factor)\n",
        "        else:\n",
        "            data = data[:, :, start_width//2:start_width//2+bin_size]\n",
        "\n",
        "    else:\n",
        "        data = data[:, :, start_width//2:start_width//2+bin_size]\n",
        "\n",
        "    if model_type == \"conv\":\n",
        "        data = data[:, np.newaxis]\n",
        "    elif model_type == \"rec\":\n",
        "        data = data.swapaxes(1, 2)\n",
        "\n",
        "    return data, labels, np.array(trials)\n",
        "\n",
        "def GetData(noise_factor=0, slide_factor=1, subsample_factor=1, bin_size=1000, augment_train=False, augment_test=False, model_type=\"conv\"):\n",
        "    X_test = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/X_test.npy\")\n",
        "    y_test = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/y_test.npy\")\n",
        "    person_train_valid = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/person_train_valid.npy\")\n",
        "    X_train_valid = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/X_train_valid.npy\")\n",
        "    y_train_valid = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/y_train_valid.npy\")\n",
        "    person_test = np.load(\"/content/gdrive/My Drive/Colab Notebooks/project/person_test.npy\")\n",
        "\n",
        "    class_map = {769:0, 770:1, 771:2, 772:3}\n",
        "    y_train_valid = np.array([class_map[x] for x in y_train_valid])\n",
        "    y_test = np.array([class_map[x] for x in y_test])\n",
        "\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, stratify=y_train_valid, random_state=0)\n",
        "    X_train, y_train, train_trials = AugmentData(X_train, y_train, noise_factor, slide_factor, subsample_factor, bin_size, augment=augment_train, train_data=True, model_type=model_type)\n",
        "    X_valid, y_valid, valid_trials = AugmentData(X_valid, y_valid, noise_factor, slide_factor, subsample_factor, bin_size, augment=augment_test, train_data=False, model_type=model_type)\n",
        "    X_test, y_test, test_trials = AugmentData(X_test, y_test, noise_factor, slide_factor, subsample_factor, bin_size, augment=augment_test, train_data=False, model_type=model_type)\n",
        "    \n",
        "    print(\"Data shape:\", X_train.shape)\n",
        "    print(\"Target shape:\", y_train.shape)\n",
        "    print(\"Trial shape:\", train_trials.shape)\n",
        "    return X_train, y_train, X_valid, y_valid, X_test, y_test, train_trials, valid_trials, test_trials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPlMBtiniByC",
        "colab_type": "code",
        "outputId": "4f983ebe-085c-40a9-f016-a49c54239ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls \"/content/gdrive/My Drive/Colab Notebooks/project/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_loading.ipynb  person_train_valid.npy  X_train_valid.npy  y_train_valid.npy\n",
            "person_test.npy    X_test.npy\t\t   y_test.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7XmIAellLqa",
        "colab_type": "code",
        "outputId": "7ccbaf9c-9a05-4be6-f9d6-422e462de13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import torch\n",
        "import torch.utils.data as utils\n",
        "from torch import optim\n",
        "import argparse\n",
        "\n",
        "def ClassifySVM(X_train_valid, y_train_valid):\n",
        "    X_train_valid = X_train_valid.reshape((len(X_train_valid), -1))\n",
        "    print(\"Data shape:\", X_train_valid.shape)\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, stratify=y_train_valid, random_state=0)\n",
        "\n",
        "    svm = SVC()\n",
        "    svm.fit(X_train, y_train)\n",
        "    train_acc = np.mean(svm.predict(X_train) == y_train)\n",
        "    scores = svm.predict(X_valid)\n",
        "    val_acc = np.mean(scores == y_valid)\n",
        "\n",
        "    print(\"Train acc:\", train_acc)\n",
        "    print(\"Val acc:\", val_acc)\n",
        "\n",
        "def GetTensorLoader(data, labels, trials=None, batch_size=200):\n",
        "    data_tensor = torch.Tensor(data)\n",
        "    label_tensor = torch.Tensor(labels).long()\n",
        "    if trials is None:\n",
        "        dataset = utils.TensorDataset(data_tensor, label_tensor)\n",
        "    else:\n",
        "        trial_tensor = torch.Tensor(trials).long()\n",
        "        dataset = utils.TensorDataset(data_tensor, label_tensor, trial_tensor)\n",
        "    loader = utils.DataLoader(dataset, batch_size=batch_size)\n",
        "    return loader\n",
        "\n",
        "def main(bin_size, num_epochs, filter_width, augment_train, augment_test, slide_factor, noise_factor, subsample_factor, model_type):\n",
        "    if model_type != \"conv\" and model_type != \"rec\" and model_type != \"other\":\n",
        "        print(\"Model type must be either 'conv', 'rec', or 'other'. Got:\", model_type)\n",
        "        exit(1)\n",
        "\n",
        "    use_gpu = True\n",
        "\n",
        "    X_train, y_train, X_valid, y_valid, X_test, y_test, train_trials, valid_trials, test_trials = \\\n",
        "        GetData(noise_factor, slide_factor, subsample_factor, bin_size, augment_train, augment_test, model_type)\n",
        "\n",
        "    #TODO: reshape your data here as needed\n",
        "\n",
        "    train_loader = GetTensorLoader(X_train, y_train, train_trials, batch_size=200)\n",
        "    valid_loader = GetTensorLoader(X_valid, y_valid, valid_trials, batch_size=200)\n",
        "    \n",
        "    #TODO: specify your network here\n",
        "\n",
        "    net = CNN_Schirr(input_size = bin_size, filter_width = filter_width, dropout_rate = 0.4)\n",
        "    #net = RNN(bin_size)\n",
        "    optimizer = optim.Adam(net.parameters(), net.lr, weight_decay=net.weight_decay) # TODO: determine best optimizer/params\n",
        "    print(\"Number of parameters for {}: {}\\n\".format(net.name, sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
        "    TrainTestModel(net, optimizer, train_loader, valid_loader, num_epochs=num_epochs, use_gpu=use_gpu, get_trial_acc=augment_test)\n",
        "\n",
        "main(bin_size = 1000, num_epochs = 200, augment_train = True, augment_test = True , \\\n",
        "     slide_factor = 1, noise_factor = 0, subsample_factor = 1, model_type = 'conv', filter_width = 7)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape: (1692, 1, 22, 1000)\n",
            "Target shape: (1692,)\n",
            "Trial shape: (1692,)\n",
            "Number of parameters for CNN_Schirr: 192680\n",
            "\n",
            "Running epoch: 0\n",
            "Iter 200 - loss: 2.0151, acc: 0.1000\n",
            "Iter 400 - loss: 1.8283, acc: 0.1450\n",
            "Iter 600 - loss: 1.7143, acc: 0.1800\n",
            "Iter 800 - loss: 1.7499, acc: 0.1600\n",
            "Iter 1000 - loss: 1.6280, acc: 0.2700\n",
            "Iter 1200 - loss: 1.6240, acc: 0.2600\n",
            "Iter 1400 - loss: 1.5400, acc: 0.2900\n",
            "Iter 1600 - loss: 1.5696, acc: 0.2350\n",
            "Train set: Average loss: 1.6991, Accuracy: 354/1692 (20.92%)\n",
            "Train Trial Accuracy: 26.71%\n",
            "\n",
            "Test set: Average loss: 1.6517, Accuracy: 106/423 (25.06%)\n",
            "Test Trial Accuracy: 25.06%\n",
            "\n",
            "Running epoch: 1\n",
            "Iter 200 - loss: 1.4733, acc: 0.3400\n",
            "Iter 400 - loss: 1.5055, acc: 0.3000\n",
            "Iter 600 - loss: 1.4799, acc: 0.2450\n",
            "Iter 800 - loss: 1.4141, acc: 0.3050\n",
            "Iter 1000 - loss: 1.4481, acc: 0.3050\n",
            "Iter 1200 - loss: 1.4487, acc: 0.3400\n",
            "Iter 1400 - loss: 1.4652, acc: 0.2800\n",
            "Iter 1600 - loss: 1.4123, acc: 0.2950\n",
            "Train set: Average loss: 1.4560, Accuracy: 511/1692 (30.20%)\n",
            "Train Trial Accuracy: 30.20%\n",
            "\n",
            "Test set: Average loss: 1.5218, Accuracy: 102/423 (24.11%)\n",
            "Test Trial Accuracy: 24.59%\n",
            "\n",
            "Running epoch: 2\n",
            "Iter 200 - loss: 1.4304, acc: 0.2950\n",
            "Iter 400 - loss: 1.4313, acc: 0.2750\n",
            "Iter 600 - loss: 1.4267, acc: 0.3050\n",
            "Iter 800 - loss: 1.3926, acc: 0.3700\n",
            "Iter 1000 - loss: 1.4021, acc: 0.3300\n",
            "Iter 1200 - loss: 1.3796, acc: 0.3050\n",
            "Iter 1400 - loss: 1.3979, acc: 0.3150\n",
            "Iter 1600 - loss: 1.4165, acc: 0.2900\n",
            "Train set: Average loss: 1.4094, Accuracy: 522/1692 (30.85%)\n",
            "Train Trial Accuracy: 30.85%\n",
            "\n",
            "Test set: Average loss: 1.4352, Accuracy: 139/423 (32.86%)\n",
            "Test Trial Accuracy: 32.86%\n",
            "\n",
            "Running epoch: 3\n",
            "Iter 200 - loss: 1.3684, acc: 0.3500\n",
            "Iter 400 - loss: 1.3577, acc: 0.3250\n",
            "Iter 600 - loss: 1.3666, acc: 0.3400\n",
            "Iter 800 - loss: 1.3432, acc: 0.3300\n",
            "Iter 1000 - loss: 1.3964, acc: 0.3050\n",
            "Iter 1200 - loss: 1.3888, acc: 0.3200\n",
            "Iter 1400 - loss: 1.3456, acc: 0.3550\n",
            "Iter 1600 - loss: 1.3234, acc: 0.3200\n",
            "Train set: Average loss: 1.3623, Accuracy: 560/1692 (33.10%)\n",
            "Train Trial Accuracy: 33.10%\n",
            "\n",
            "Test set: Average loss: 1.4150, Accuracy: 141/423 (33.33%)\n",
            "Test Trial Accuracy: 33.33%\n",
            "\n",
            "Running epoch: 4\n",
            "Iter 200 - loss: 1.3457, acc: 0.3100\n",
            "Iter 400 - loss: 1.3737, acc: 0.3200\n",
            "Iter 600 - loss: 1.3221, acc: 0.3750\n",
            "Iter 800 - loss: 1.3364, acc: 0.3750\n",
            "Iter 1000 - loss: 1.3446, acc: 0.3500\n",
            "Iter 1200 - loss: 1.3314, acc: 0.3750\n",
            "Iter 1400 - loss: 1.3351, acc: 0.3300\n",
            "Iter 1600 - loss: 1.3077, acc: 0.4050\n",
            "Train set: Average loss: 1.3333, Accuracy: 602/1692 (35.58%)\n",
            "Train Trial Accuracy: 35.58%\n",
            "\n",
            "Test set: Average loss: 1.4068, Accuracy: 132/423 (31.21%)\n",
            "Test Trial Accuracy: 31.21%\n",
            "\n",
            "Running epoch: 5\n",
            "Iter 200 - loss: 1.3310, acc: 0.3700\n",
            "Iter 400 - loss: 1.3395, acc: 0.3400\n",
            "Iter 600 - loss: 1.3350, acc: 0.3450\n",
            "Iter 800 - loss: 1.3140, acc: 0.3850\n",
            "Iter 1000 - loss: 1.3350, acc: 0.3700\n",
            "Iter 1200 - loss: 1.3235, acc: 0.3700\n",
            "Iter 1400 - loss: 1.3007, acc: 0.3750\n",
            "Iter 1600 - loss: 1.3151, acc: 0.3500\n",
            "Train set: Average loss: 1.3223, Accuracy: 619/1692 (36.58%)\n",
            "Train Trial Accuracy: 36.58%\n",
            "\n",
            "Test set: Average loss: 1.4058, Accuracy: 138/423 (32.62%)\n",
            "Test Trial Accuracy: 32.62%\n",
            "\n",
            "Running epoch: 6\n",
            "Iter 200 - loss: 1.3402, acc: 0.3150\n",
            "Iter 400 - loss: 1.2843, acc: 0.3900\n",
            "Iter 600 - loss: 1.2917, acc: 0.3950\n",
            "Iter 800 - loss: 1.2238, acc: 0.4650\n",
            "Iter 1000 - loss: 1.2920, acc: 0.4000\n",
            "Iter 1200 - loss: 1.3048, acc: 0.3800\n",
            "Iter 1400 - loss: 1.2663, acc: 0.4350\n",
            "Iter 1600 - loss: 1.2649, acc: 0.4200\n",
            "Train set: Average loss: 1.2881, Accuracy: 671/1692 (39.66%)\n",
            "Train Trial Accuracy: 39.66%\n",
            "\n",
            "Test set: Average loss: 1.3784, Accuracy: 139/423 (32.86%)\n",
            "Test Trial Accuracy: 32.86%\n",
            "\n",
            "Running epoch: 7\n",
            "Iter 200 - loss: 1.2684, acc: 0.4300\n",
            "Iter 400 - loss: 1.2818, acc: 0.3650\n",
            "Iter 600 - loss: 1.2597, acc: 0.4150\n",
            "Iter 800 - loss: 1.1920, acc: 0.4400\n",
            "Iter 1000 - loss: 1.3151, acc: 0.3850\n",
            "Iter 1200 - loss: 1.2591, acc: 0.3750\n",
            "Iter 1400 - loss: 1.2402, acc: 0.4250\n",
            "Iter 1600 - loss: 1.2599, acc: 0.4100\n",
            "Train set: Average loss: 1.2595, Accuracy: 688/1692 (40.66%)\n",
            "Train Trial Accuracy: 40.66%\n",
            "\n",
            "Test set: Average loss: 1.3553, Accuracy: 148/423 (34.99%)\n",
            "Test Trial Accuracy: 34.99%\n",
            "\n",
            "Running epoch: 8\n",
            "Iter 200 - loss: 1.2535, acc: 0.4350\n",
            "Iter 400 - loss: 1.2268, acc: 0.4200\n",
            "Iter 600 - loss: 1.2313, acc: 0.4550\n",
            "Iter 800 - loss: 1.1868, acc: 0.4950\n",
            "Iter 1000 - loss: 1.2813, acc: 0.4550\n",
            "Iter 1200 - loss: 1.2639, acc: 0.4000\n",
            "Iter 1400 - loss: 1.2361, acc: 0.4600\n",
            "Iter 1600 - loss: 1.2517, acc: 0.3900\n",
            "Train set: Average loss: 1.2403, Accuracy: 743/1692 (43.91%)\n",
            "Train Trial Accuracy: 43.91%\n",
            "\n",
            "Test set: Average loss: 1.2974, Accuracy: 178/423 (42.08%)\n",
            "Test Trial Accuracy: 42.08%\n",
            "\n",
            "Running epoch: 9\n",
            "Iter 200 - loss: 1.2402, acc: 0.4250\n",
            "Iter 400 - loss: 1.2321, acc: 0.4150\n",
            "Iter 600 - loss: 1.2075, acc: 0.4500\n",
            "Iter 800 - loss: 1.1344, acc: 0.5050\n",
            "Iter 1000 - loss: 1.3031, acc: 0.4500\n",
            "Iter 1200 - loss: 1.1928, acc: 0.4500\n",
            "Iter 1400 - loss: 1.2359, acc: 0.3950\n",
            "Iter 1600 - loss: 1.2554, acc: 0.3850\n",
            "Train set: Average loss: 1.2227, Accuracy: 735/1692 (43.44%)\n",
            "Train Trial Accuracy: 43.44%\n",
            "\n",
            "Test set: Average loss: 1.2787, Accuracy: 182/423 (43.03%)\n",
            "Test Trial Accuracy: 43.03%\n",
            "\n",
            "Running epoch: 10\n",
            "Iter 200 - loss: 1.1897, acc: 0.4200\n",
            "Iter 400 - loss: 1.1576, acc: 0.5050\n",
            "Iter 600 - loss: 1.1812, acc: 0.4450\n",
            "Iter 800 - loss: 1.1014, acc: 0.5300\n",
            "Iter 1000 - loss: 1.2432, acc: 0.4500\n",
            "Iter 1200 - loss: 1.2025, acc: 0.4600\n",
            "Iter 1400 - loss: 1.2316, acc: 0.4400\n",
            "Iter 1600 - loss: 1.2180, acc: 0.4300\n",
            "Train set: Average loss: 1.1874, Accuracy: 784/1692 (46.34%)\n",
            "Train Trial Accuracy: 46.34%\n",
            "\n",
            "Test set: Average loss: 1.2538, Accuracy: 187/423 (44.21%)\n",
            "Test Trial Accuracy: 44.21%\n",
            "\n",
            "Running epoch: 11\n",
            "Iter 200 - loss: 1.2261, acc: 0.3750\n",
            "Iter 400 - loss: 1.1757, acc: 0.4400\n",
            "Iter 600 - loss: 1.1641, acc: 0.4550\n",
            "Iter 800 - loss: 1.1010, acc: 0.5500\n",
            "Iter 1000 - loss: 1.2379, acc: 0.4750\n",
            "Iter 1200 - loss: 1.1807, acc: 0.4750\n",
            "Iter 1400 - loss: 1.1801, acc: 0.4550\n",
            "Iter 1600 - loss: 1.2088, acc: 0.4600\n",
            "Train set: Average loss: 1.1819, Accuracy: 779/1692 (46.04%)\n",
            "Train Trial Accuracy: 46.04%\n",
            "\n",
            "Test set: Average loss: 1.2578, Accuracy: 182/423 (43.03%)\n",
            "Test Trial Accuracy: 43.03%\n",
            "\n",
            "Running epoch: 12\n",
            "Iter 200 - loss: 1.1896, acc: 0.4500\n",
            "Iter 400 - loss: 1.1792, acc: 0.4350\n",
            "Iter 600 - loss: 1.1588, acc: 0.4900\n",
            "Iter 800 - loss: 1.0864, acc: 0.5050\n",
            "Iter 1000 - loss: 1.2461, acc: 0.4600\n",
            "Iter 1200 - loss: 1.1544, acc: 0.4900\n",
            "Iter 1400 - loss: 1.1875, acc: 0.4600\n",
            "Iter 1600 - loss: 1.1824, acc: 0.4900\n",
            "Train set: Average loss: 1.1692, Accuracy: 808/1692 (47.75%)\n",
            "Train Trial Accuracy: 47.75%\n",
            "\n",
            "Test set: Average loss: 1.2245, Accuracy: 192/423 (45.39%)\n",
            "Test Trial Accuracy: 45.39%\n",
            "\n",
            "Running epoch: 13\n",
            "Iter 200 - loss: 1.1359, acc: 0.4650\n",
            "Iter 400 - loss: 1.1500, acc: 0.5000\n",
            "Iter 600 - loss: 1.1372, acc: 0.5150\n",
            "Iter 800 - loss: 1.0950, acc: 0.5500\n",
            "Iter 1000 - loss: 1.2243, acc: 0.4550\n",
            "Iter 1200 - loss: 1.1180, acc: 0.5200\n",
            "Iter 1400 - loss: 1.1260, acc: 0.5000\n",
            "Iter 1600 - loss: 1.1541, acc: 0.4650\n",
            "Train set: Average loss: 1.1364, Accuracy: 846/1692 (50.00%)\n",
            "Train Trial Accuracy: 50.00%\n",
            "\n",
            "Test set: Average loss: 1.2112, Accuracy: 197/423 (46.57%)\n",
            "Test Trial Accuracy: 46.57%\n",
            "\n",
            "Running epoch: 14\n",
            "Iter 200 - loss: 1.1597, acc: 0.4550\n",
            "Iter 400 - loss: 1.1336, acc: 0.4900\n",
            "Iter 600 - loss: 1.0747, acc: 0.5200\n",
            "Iter 800 - loss: 1.0717, acc: 0.5350\n",
            "Iter 1000 - loss: 1.1950, acc: 0.5150\n",
            "Iter 1200 - loss: 1.1341, acc: 0.4950\n",
            "Iter 1400 - loss: 1.1375, acc: 0.5050\n",
            "Iter 1600 - loss: 1.1290, acc: 0.4550\n",
            "Train set: Average loss: 1.1276, Accuracy: 840/1692 (49.65%)\n",
            "Train Trial Accuracy: 49.65%\n",
            "\n",
            "Test set: Average loss: 1.2298, Accuracy: 185/423 (43.74%)\n",
            "Test Trial Accuracy: 43.74%\n",
            "\n",
            "Running epoch: 15\n",
            "Iter 200 - loss: 1.1115, acc: 0.5400\n",
            "Iter 400 - loss: 1.1352, acc: 0.4750\n",
            "Iter 600 - loss: 1.1288, acc: 0.5300\n",
            "Iter 800 - loss: 1.0763, acc: 0.5200\n",
            "Iter 1000 - loss: 1.1598, acc: 0.5450\n",
            "Iter 1200 - loss: 1.0924, acc: 0.5150\n",
            "Iter 1400 - loss: 1.1773, acc: 0.4850\n",
            "Iter 1600 - loss: 1.1298, acc: 0.4650\n",
            "Train set: Average loss: 1.1175, Accuracy: 874/1692 (51.65%)\n",
            "Train Trial Accuracy: 51.65%\n",
            "\n",
            "Test set: Average loss: 1.2112, Accuracy: 193/423 (45.63%)\n",
            "Test Trial Accuracy: 45.63%\n",
            "\n",
            "Running epoch: 16\n",
            "Iter 200 - loss: 1.0913, acc: 0.5000\n",
            "Iter 400 - loss: 1.1600, acc: 0.4950\n",
            "Iter 600 - loss: 1.1222, acc: 0.5300\n",
            "Iter 800 - loss: 1.0683, acc: 0.5100\n",
            "Iter 1000 - loss: 1.1506, acc: 0.5550\n",
            "Iter 1200 - loss: 1.0739, acc: 0.5600\n",
            "Iter 1400 - loss: 1.1570, acc: 0.4850\n",
            "Iter 1600 - loss: 1.1172, acc: 0.5250\n",
            "Train set: Average loss: 1.1136, Accuracy: 882/1692 (52.13%)\n",
            "Train Trial Accuracy: 52.13%\n",
            "\n",
            "Test set: Average loss: 1.1973, Accuracy: 203/423 (47.99%)\n",
            "Test Trial Accuracy: 47.99%\n",
            "\n",
            "Running epoch: 17\n",
            "Iter 200 - loss: 1.0555, acc: 0.5700\n",
            "Iter 400 - loss: 1.0760, acc: 0.5150\n",
            "Iter 600 - loss: 1.0241, acc: 0.5700\n",
            "Iter 800 - loss: 1.0502, acc: 0.5250\n",
            "Iter 1000 - loss: 1.1541, acc: 0.5500\n",
            "Iter 1200 - loss: 1.0607, acc: 0.5450\n",
            "Iter 1400 - loss: 1.1150, acc: 0.5250\n",
            "Iter 1600 - loss: 1.0909, acc: 0.5500\n",
            "Train set: Average loss: 1.0732, Accuracy: 930/1692 (54.96%)\n",
            "Train Trial Accuracy: 54.96%\n",
            "\n",
            "Test set: Average loss: 1.1707, Accuracy: 207/423 (48.94%)\n",
            "Test Trial Accuracy: 48.94%\n",
            "\n",
            "Running epoch: 18\n",
            "Iter 200 - loss: 1.0880, acc: 0.5550\n",
            "Iter 400 - loss: 1.0360, acc: 0.5650\n",
            "Iter 600 - loss: 1.0049, acc: 0.6050\n",
            "Iter 800 - loss: 1.0021, acc: 0.5800\n",
            "Iter 1000 - loss: 1.1176, acc: 0.5650\n",
            "Iter 1200 - loss: 1.0305, acc: 0.5200\n",
            "Iter 1400 - loss: 1.0924, acc: 0.5250\n",
            "Iter 1600 - loss: 1.0443, acc: 0.5400\n",
            "Train set: Average loss: 1.0461, Accuracy: 950/1692 (56.15%)\n",
            "Train Trial Accuracy: 56.15%\n",
            "\n",
            "Test set: Average loss: 1.1559, Accuracy: 203/423 (47.99%)\n",
            "Test Trial Accuracy: 47.99%\n",
            "\n",
            "Running epoch: 19\n",
            "Iter 200 - loss: 1.0375, acc: 0.5500\n",
            "Iter 400 - loss: 1.0701, acc: 0.5300\n",
            "Iter 600 - loss: 0.9461, acc: 0.6200\n",
            "Iter 800 - loss: 0.9374, acc: 0.6400\n",
            "Iter 1000 - loss: 1.1238, acc: 0.5600\n",
            "Iter 1200 - loss: 1.0448, acc: 0.5650\n",
            "Iter 1400 - loss: 1.0100, acc: 0.5800\n",
            "Iter 1600 - loss: 1.0244, acc: 0.5600\n",
            "Train set: Average loss: 1.0178, Accuracy: 985/1692 (58.22%)\n",
            "Train Trial Accuracy: 58.22%\n",
            "\n",
            "Test set: Average loss: 1.1444, Accuracy: 218/423 (51.54%)\n",
            "Test Trial Accuracy: 51.54%\n",
            "\n",
            "Running epoch: 20\n",
            "Iter 200 - loss: 1.0168, acc: 0.5700\n",
            "Iter 400 - loss: 1.0345, acc: 0.5750\n",
            "Iter 600 - loss: 0.9642, acc: 0.6000\n",
            "Iter 800 - loss: 0.9258, acc: 0.6150\n",
            "Iter 1000 - loss: 1.0996, acc: 0.5100\n",
            "Iter 1200 - loss: 1.0175, acc: 0.5750\n",
            "Iter 1400 - loss: 1.0522, acc: 0.5900\n",
            "Iter 1600 - loss: 1.0017, acc: 0.5500\n",
            "Train set: Average loss: 1.0052, Accuracy: 982/1692 (58.04%)\n",
            "Train Trial Accuracy: 58.04%\n",
            "\n",
            "Test set: Average loss: 1.1101, Accuracy: 220/423 (52.01%)\n",
            "Test Trial Accuracy: 52.01%\n",
            "\n",
            "Running epoch: 21\n",
            "Iter 200 - loss: 0.9427, acc: 0.6050\n",
            "Iter 400 - loss: 0.9838, acc: 0.5900\n",
            "Iter 600 - loss: 0.9535, acc: 0.6250\n",
            "Iter 800 - loss: 0.9370, acc: 0.6200\n",
            "Iter 1000 - loss: 1.1094, acc: 0.5150\n",
            "Iter 1200 - loss: 0.9842, acc: 0.5950\n",
            "Iter 1400 - loss: 1.0572, acc: 0.5500\n",
            "Iter 1600 - loss: 0.9957, acc: 0.5550\n",
            "Train set: Average loss: 0.9866, Accuracy: 994/1692 (58.75%)\n",
            "Train Trial Accuracy: 58.75%\n",
            "\n",
            "Test set: Average loss: 1.1055, Accuracy: 221/423 (52.25%)\n",
            "Test Trial Accuracy: 52.25%\n",
            "\n",
            "Running epoch: 22\n",
            "Iter 200 - loss: 0.9767, acc: 0.5800\n",
            "Iter 400 - loss: 1.0028, acc: 0.5450\n",
            "Iter 600 - loss: 0.9369, acc: 0.6400\n",
            "Iter 800 - loss: 0.9405, acc: 0.5950\n",
            "Iter 1000 - loss: 1.0147, acc: 0.6100\n",
            "Iter 1200 - loss: 0.9473, acc: 0.5950\n",
            "Iter 1400 - loss: 1.0154, acc: 0.5700\n",
            "Iter 1600 - loss: 0.9519, acc: 0.6000\n",
            "Train set: Average loss: 0.9651, Accuracy: 1011/1692 (59.75%)\n",
            "Train Trial Accuracy: 59.75%\n",
            "\n",
            "Test set: Average loss: 1.0744, Accuracy: 243/423 (57.45%)\n",
            "Test Trial Accuracy: 57.45%\n",
            "\n",
            "Running epoch: 23\n",
            "Iter 200 - loss: 0.8724, acc: 0.6550\n",
            "Iter 400 - loss: 0.9200, acc: 0.6150\n",
            "Iter 600 - loss: 0.8703, acc: 0.6550\n",
            "Iter 800 - loss: 0.9341, acc: 0.5850\n",
            "Iter 1000 - loss: 1.0619, acc: 0.6050\n",
            "Iter 1200 - loss: 0.9324, acc: 0.6300\n",
            "Iter 1400 - loss: 0.9876, acc: 0.5700\n",
            "Iter 1600 - loss: 0.9740, acc: 0.5750\n",
            "Train set: Average loss: 0.9359, Accuracy: 1046/1692 (61.82%)\n",
            "Train Trial Accuracy: 61.82%\n",
            "\n",
            "Test set: Average loss: 1.0653, Accuracy: 235/423 (55.56%)\n",
            "Test Trial Accuracy: 55.56%\n",
            "\n",
            "Running epoch: 24\n",
            "Iter 200 - loss: 0.9362, acc: 0.6500\n",
            "Iter 400 - loss: 0.8916, acc: 0.6300\n",
            "Iter 600 - loss: 0.8300, acc: 0.7050\n",
            "Iter 800 - loss: 0.8467, acc: 0.6900\n",
            "Iter 1000 - loss: 1.0418, acc: 0.5950\n",
            "Iter 1200 - loss: 0.9131, acc: 0.6200\n",
            "Iter 1400 - loss: 0.9369, acc: 0.6250\n",
            "Iter 1600 - loss: 0.9300, acc: 0.6200\n",
            "Train set: Average loss: 0.9090, Accuracy: 1096/1692 (64.78%)\n",
            "Train Trial Accuracy: 64.78%\n",
            "\n",
            "Test set: Average loss: 1.0406, Accuracy: 236/423 (55.79%)\n",
            "Test Trial Accuracy: 55.79%\n",
            "\n",
            "Running epoch: 25\n",
            "Iter 200 - loss: 0.8456, acc: 0.6950\n",
            "Iter 400 - loss: 0.8831, acc: 0.6500\n",
            "Iter 600 - loss: 0.8339, acc: 0.6900\n",
            "Iter 800 - loss: 0.8676, acc: 0.6700\n",
            "Iter 1000 - loss: 1.0645, acc: 0.5300\n",
            "Iter 1200 - loss: 0.9182, acc: 0.6300\n",
            "Iter 1400 - loss: 0.9421, acc: 0.6150\n",
            "Iter 1600 - loss: 0.8896, acc: 0.6250\n",
            "Train set: Average loss: 0.8975, Accuracy: 1084/1692 (64.07%)\n",
            "Train Trial Accuracy: 64.07%\n",
            "\n",
            "Test set: Average loss: 1.0600, Accuracy: 238/423 (56.26%)\n",
            "Test Trial Accuracy: 56.26%\n",
            "\n",
            "Running epoch: 26\n",
            "Iter 200 - loss: 0.8808, acc: 0.6650\n",
            "Iter 400 - loss: 0.8943, acc: 0.6150\n",
            "Iter 600 - loss: 0.7510, acc: 0.7300\n",
            "Iter 800 - loss: 0.8661, acc: 0.6700\n",
            "Iter 1000 - loss: 1.0427, acc: 0.5800\n",
            "Iter 1200 - loss: 0.8608, acc: 0.6600\n",
            "Iter 1400 - loss: 0.9097, acc: 0.6450\n",
            "Iter 1600 - loss: 0.9071, acc: 0.6050\n",
            "Train set: Average loss: 0.8774, Accuracy: 1105/1692 (65.31%)\n",
            "Train Trial Accuracy: 65.31%\n",
            "\n",
            "Test set: Average loss: 0.9997, Accuracy: 253/423 (59.81%)\n",
            "Test Trial Accuracy: 59.81%\n",
            "\n",
            "Running epoch: 27\n",
            "Iter 200 - loss: 0.8416, acc: 0.6300\n",
            "Iter 400 - loss: 0.8573, acc: 0.6400\n",
            "Iter 600 - loss: 0.7890, acc: 0.6800\n",
            "Iter 800 - loss: 0.8302, acc: 0.6500\n",
            "Iter 1000 - loss: 0.9830, acc: 0.6100\n",
            "Iter 1200 - loss: 0.9156, acc: 0.6400\n",
            "Iter 1400 - loss: 0.8872, acc: 0.6250\n",
            "Iter 1600 - loss: 0.8821, acc: 0.6400\n",
            "Train set: Average loss: 0.8619, Accuracy: 1095/1692 (64.72%)\n",
            "Train Trial Accuracy: 64.72%\n",
            "\n",
            "Test set: Average loss: 1.0173, Accuracy: 235/423 (55.56%)\n",
            "Test Trial Accuracy: 55.56%\n",
            "\n",
            "Running epoch: 28\n",
            "Iter 200 - loss: 0.8339, acc: 0.6750\n",
            "Iter 400 - loss: 0.8441, acc: 0.6950\n",
            "Iter 600 - loss: 0.8317, acc: 0.6850\n",
            "Iter 800 - loss: 0.8037, acc: 0.6700\n",
            "Iter 1000 - loss: 0.9418, acc: 0.6100\n",
            "Iter 1200 - loss: 0.9056, acc: 0.6400\n",
            "Iter 1400 - loss: 0.8906, acc: 0.6550\n",
            "Iter 1600 - loss: 0.8544, acc: 0.6250\n",
            "Train set: Average loss: 0.8536, Accuracy: 1121/1692 (66.25%)\n",
            "Train Trial Accuracy: 66.25%\n",
            "\n",
            "Test set: Average loss: 0.9660, Accuracy: 251/423 (59.34%)\n",
            "Test Trial Accuracy: 59.34%\n",
            "\n",
            "Running epoch: 29\n",
            "Iter 200 - loss: 0.8534, acc: 0.6300\n",
            "Iter 400 - loss: 0.7914, acc: 0.7150\n",
            "Iter 600 - loss: 0.8017, acc: 0.6600\n",
            "Iter 800 - loss: 0.8343, acc: 0.6800\n",
            "Iter 1000 - loss: 0.9656, acc: 0.6000\n",
            "Iter 1200 - loss: 0.8575, acc: 0.6350\n",
            "Iter 1400 - loss: 0.8981, acc: 0.6400\n",
            "Iter 1600 - loss: 0.8484, acc: 0.6050\n",
            "Train set: Average loss: 0.8476, Accuracy: 1101/1692 (65.07%)\n",
            "Train Trial Accuracy: 65.07%\n",
            "\n",
            "Test set: Average loss: 0.9801, Accuracy: 259/423 (61.23%)\n",
            "Test Trial Accuracy: 61.23%\n",
            "\n",
            "Running epoch: 30\n",
            "Iter 200 - loss: 0.8484, acc: 0.6750\n",
            "Iter 400 - loss: 0.8124, acc: 0.6750\n",
            "Iter 600 - loss: 0.7649, acc: 0.7150\n",
            "Iter 800 - loss: 0.7726, acc: 0.6800\n",
            "Iter 1000 - loss: 0.9376, acc: 0.6350\n",
            "Iter 1200 - loss: 0.8302, acc: 0.6750\n",
            "Iter 1400 - loss: 0.9192, acc: 0.5950\n",
            "Iter 1600 - loss: 0.8283, acc: 0.6600\n",
            "Train set: Average loss: 0.8309, Accuracy: 1135/1692 (67.08%)\n",
            "Train Trial Accuracy: 67.08%\n",
            "\n",
            "Test set: Average loss: 0.9745, Accuracy: 268/423 (63.36%)\n",
            "Test Trial Accuracy: 63.36%\n",
            "\n",
            "Running epoch: 31\n",
            "Iter 200 - loss: 0.8729, acc: 0.6700\n",
            "Iter 400 - loss: 0.7925, acc: 0.7200\n",
            "Iter 600 - loss: 0.7738, acc: 0.6950\n",
            "Iter 800 - loss: 0.7726, acc: 0.6850\n",
            "Iter 1000 - loss: 0.9143, acc: 0.6500\n",
            "Iter 1200 - loss: 0.8018, acc: 0.6900\n",
            "Iter 1400 - loss: 0.8032, acc: 0.6700\n",
            "Iter 1600 - loss: 0.7495, acc: 0.7100\n",
            "Train set: Average loss: 0.8003, Accuracy: 1171/1692 (69.21%)\n",
            "Train Trial Accuracy: 69.21%\n",
            "\n",
            "Test set: Average loss: 1.0969, Accuracy: 228/423 (53.90%)\n",
            "Test Trial Accuracy: 53.90%\n",
            "\n",
            "Running epoch: 32\n",
            "Iter 200 - loss: 0.8283, acc: 0.6700\n",
            "Iter 400 - loss: 0.8102, acc: 0.6550\n",
            "Iter 600 - loss: 0.7060, acc: 0.7250\n",
            "Iter 800 - loss: 0.7656, acc: 0.7250\n",
            "Iter 1000 - loss: 0.9060, acc: 0.6800\n",
            "Iter 1200 - loss: 0.8456, acc: 0.6750\n",
            "Iter 1400 - loss: 0.8118, acc: 0.7100\n",
            "Iter 1600 - loss: 0.8383, acc: 0.6450\n",
            "Train set: Average loss: 0.7991, Accuracy: 1173/1692 (69.33%)\n",
            "Train Trial Accuracy: 69.33%\n",
            "\n",
            "Test set: Average loss: 0.9588, Accuracy: 273/423 (64.54%)\n",
            "Test Trial Accuracy: 64.54%\n",
            "\n",
            "Running epoch: 33\n",
            "Iter 200 - loss: 0.7785, acc: 0.7050\n",
            "Iter 400 - loss: 0.7784, acc: 0.6950\n",
            "Iter 600 - loss: 0.6911, acc: 0.7450\n",
            "Iter 800 - loss: 0.7121, acc: 0.7250\n",
            "Iter 1000 - loss: 0.9804, acc: 0.5900\n",
            "Iter 1200 - loss: 0.7447, acc: 0.7100\n",
            "Iter 1400 - loss: 0.7786, acc: 0.6750\n",
            "Iter 1600 - loss: 0.7024, acc: 0.7450\n",
            "Train set: Average loss: 0.7576, Accuracy: 1200/1692 (70.92%)\n",
            "Train Trial Accuracy: 70.92%\n",
            "\n",
            "Test set: Average loss: 0.9147, Accuracy: 267/423 (63.12%)\n",
            "Test Trial Accuracy: 63.12%\n",
            "\n",
            "Running epoch: 34\n",
            "Iter 200 - loss: 0.7385, acc: 0.7200\n",
            "Iter 400 - loss: 0.7136, acc: 0.7250\n",
            "Iter 600 - loss: 0.6830, acc: 0.7350\n",
            "Iter 800 - loss: 0.6848, acc: 0.7350\n",
            "Iter 1000 - loss: 0.9519, acc: 0.6350\n",
            "Iter 1200 - loss: 0.7406, acc: 0.7100\n",
            "Iter 1400 - loss: 0.8054, acc: 0.6650\n",
            "Iter 1600 - loss: 0.7494, acc: 0.7000\n",
            "Train set: Average loss: 0.7470, Accuracy: 1199/1692 (70.86%)\n",
            "Train Trial Accuracy: 70.86%\n",
            "\n",
            "Test set: Average loss: 0.8986, Accuracy: 268/423 (63.36%)\n",
            "Test Trial Accuracy: 63.36%\n",
            "\n",
            "Running epoch: 35\n",
            "Iter 200 - loss: 0.7712, acc: 0.7300\n",
            "Iter 400 - loss: 0.7282, acc: 0.7200\n",
            "Iter 600 - loss: 0.6858, acc: 0.7450\n",
            "Iter 800 - loss: 0.7048, acc: 0.7250\n",
            "Iter 1000 - loss: 0.8122, acc: 0.6900\n",
            "Iter 1200 - loss: 0.7031, acc: 0.7400\n",
            "Iter 1400 - loss: 0.7259, acc: 0.6950\n",
            "Iter 1600 - loss: 0.7463, acc: 0.7100\n",
            "Train set: Average loss: 0.7234, Accuracy: 1225/1692 (72.40%)\n",
            "Train Trial Accuracy: 72.40%\n",
            "\n",
            "Test set: Average loss: 0.9701, Accuracy: 253/423 (59.81%)\n",
            "Test Trial Accuracy: 59.81%\n",
            "\n",
            "Running epoch: 36\n",
            "Iter 200 - loss: 0.8071, acc: 0.6550\n",
            "Iter 400 - loss: 0.6922, acc: 0.7350\n",
            "Iter 600 - loss: 0.6339, acc: 0.7900\n",
            "Iter 800 - loss: 0.7480, acc: 0.7100\n",
            "Iter 1000 - loss: 0.8383, acc: 0.6550\n",
            "Iter 1200 - loss: 0.7261, acc: 0.7400\n",
            "Iter 1400 - loss: 0.7576, acc: 0.6950\n",
            "Iter 1600 - loss: 0.6736, acc: 0.7350\n",
            "Train set: Average loss: 0.7247, Accuracy: 1219/1692 (72.04%)\n",
            "Train Trial Accuracy: 72.04%\n",
            "\n",
            "Test set: Average loss: 0.8930, Accuracy: 275/423 (65.01%)\n",
            "Test Trial Accuracy: 65.01%\n",
            "\n",
            "Running epoch: 37\n",
            "Iter 200 - loss: 0.7261, acc: 0.7200\n",
            "Iter 400 - loss: 0.7646, acc: 0.7100\n",
            "Iter 600 - loss: 0.6735, acc: 0.7350\n",
            "Iter 800 - loss: 0.6882, acc: 0.6950\n",
            "Iter 1000 - loss: 0.8141, acc: 0.6850\n",
            "Iter 1200 - loss: 0.6993, acc: 0.7100\n",
            "Iter 1400 - loss: 0.7681, acc: 0.7100\n",
            "Iter 1600 - loss: 0.6852, acc: 0.7350\n",
            "Train set: Average loss: 0.7168, Accuracy: 1214/1692 (71.75%)\n",
            "Train Trial Accuracy: 71.75%\n",
            "\n",
            "Test set: Average loss: 0.9169, Accuracy: 266/423 (62.88%)\n",
            "Test Trial Accuracy: 62.88%\n",
            "\n",
            "Running epoch: 38\n",
            "Iter 200 - loss: 0.6873, acc: 0.7800\n",
            "Iter 400 - loss: 0.7344, acc: 0.7250\n",
            "Iter 600 - loss: 0.6480, acc: 0.7400\n",
            "Iter 800 - loss: 0.6020, acc: 0.7850\n",
            "Iter 1000 - loss: 0.8105, acc: 0.6900\n",
            "Iter 1200 - loss: 0.7001, acc: 0.7300\n",
            "Iter 1400 - loss: 0.7306, acc: 0.7150\n",
            "Iter 1600 - loss: 0.6897, acc: 0.7300\n",
            "Train set: Average loss: 0.6931, Accuracy: 1252/1692 (74.00%)\n",
            "Train Trial Accuracy: 74.00%\n",
            "\n",
            "Test set: Average loss: 0.9075, Accuracy: 266/423 (62.88%)\n",
            "Test Trial Accuracy: 62.88%\n",
            "\n",
            "Running epoch: 39\n",
            "Iter 200 - loss: 0.7069, acc: 0.7250\n",
            "Iter 400 - loss: 0.6855, acc: 0.7400\n",
            "Iter 600 - loss: 0.6199, acc: 0.7700\n",
            "Iter 800 - loss: 0.6758, acc: 0.7450\n",
            "Iter 1000 - loss: 0.8312, acc: 0.6700\n",
            "Iter 1200 - loss: 0.6925, acc: 0.7300\n",
            "Iter 1400 - loss: 0.7324, acc: 0.6950\n",
            "Iter 1600 - loss: 0.6526, acc: 0.7650\n",
            "Train set: Average loss: 0.6851, Accuracy: 1249/1692 (73.82%)\n",
            "Train Trial Accuracy: 73.82%\n",
            "\n",
            "Test set: Average loss: 0.8757, Accuracy: 287/423 (67.85%)\n",
            "Test Trial Accuracy: 67.85%\n",
            "\n",
            "Running epoch: 40\n",
            "Iter 200 - loss: 0.6862, acc: 0.7250\n",
            "Iter 400 - loss: 0.7268, acc: 0.7300\n",
            "Iter 600 - loss: 0.7051, acc: 0.7500\n",
            "Iter 800 - loss: 0.6164, acc: 0.7750\n",
            "Iter 1000 - loss: 0.8271, acc: 0.6650\n",
            "Iter 1200 - loss: 0.6687, acc: 0.7550\n",
            "Iter 1400 - loss: 0.7134, acc: 0.7250\n",
            "Iter 1600 - loss: 0.6175, acc: 0.7600\n",
            "Train set: Average loss: 0.6823, Accuracy: 1257/1692 (74.29%)\n",
            "Train Trial Accuracy: 74.29%\n",
            "\n",
            "Test set: Average loss: 0.8873, Accuracy: 275/423 (65.01%)\n",
            "Test Trial Accuracy: 65.01%\n",
            "\n",
            "Running epoch: 41\n",
            "Iter 200 - loss: 0.6436, acc: 0.7700\n",
            "Iter 400 - loss: 0.7315, acc: 0.7050\n",
            "Iter 600 - loss: 0.6103, acc: 0.7650\n",
            "Iter 800 - loss: 0.6932, acc: 0.7400\n",
            "Iter 1000 - loss: 0.7798, acc: 0.7050\n",
            "Iter 1200 - loss: 0.6748, acc: 0.7450\n",
            "Iter 1400 - loss: 0.6664, acc: 0.7550\n",
            "Iter 1600 - loss: 0.5939, acc: 0.8050\n",
            "Train set: Average loss: 0.6608, Accuracy: 1275/1692 (75.35%)\n",
            "Train Trial Accuracy: 75.35%\n",
            "\n",
            "Test set: Average loss: 0.9463, Accuracy: 260/423 (61.47%)\n",
            "Test Trial Accuracy: 61.47%\n",
            "\n",
            "Running epoch: 42\n",
            "Iter 200 - loss: 0.6943, acc: 0.7000\n",
            "Iter 400 - loss: 0.6735, acc: 0.7800\n",
            "Iter 600 - loss: 0.6133, acc: 0.7550\n",
            "Iter 800 - loss: 0.6638, acc: 0.7450\n",
            "Iter 1000 - loss: 0.8173, acc: 0.7100\n",
            "Iter 1200 - loss: 0.6430, acc: 0.7450\n",
            "Iter 1400 - loss: 0.7250, acc: 0.7250\n",
            "Iter 1600 - loss: 0.6443, acc: 0.7400\n",
            "Train set: Average loss: 0.6739, Accuracy: 1261/1692 (74.53%)\n",
            "Train Trial Accuracy: 74.53%\n",
            "\n",
            "Test set: Average loss: 1.0107, Accuracy: 247/423 (58.39%)\n",
            "Test Trial Accuracy: 58.39%\n",
            "\n",
            "Running epoch: 43\n",
            "Iter 200 - loss: 0.6595, acc: 0.7400\n",
            "Iter 400 - loss: 0.6438, acc: 0.7750\n",
            "Iter 600 - loss: 0.6637, acc: 0.7400\n",
            "Iter 800 - loss: 0.5807, acc: 0.7950\n",
            "Iter 1000 - loss: 0.8167, acc: 0.6550\n",
            "Iter 1200 - loss: 0.6500, acc: 0.7400\n",
            "Iter 1400 - loss: 0.6673, acc: 0.7400\n",
            "Iter 1600 - loss: 0.5914, acc: 0.7850\n",
            "Train set: Average loss: 0.6455, Accuracy: 1278/1692 (75.53%)\n",
            "Train Trial Accuracy: 75.53%\n",
            "\n",
            "Test set: Average loss: 1.0410, Accuracy: 230/423 (54.37%)\n",
            "Test Trial Accuracy: 54.37%\n",
            "\n",
            "Running epoch: 44\n",
            "Iter 200 - loss: 0.6368, acc: 0.7700\n",
            "Iter 400 - loss: 0.6482, acc: 0.7250\n",
            "Iter 600 - loss: 0.6608, acc: 0.7250\n",
            "Iter 800 - loss: 0.6009, acc: 0.7800\n",
            "Iter 1000 - loss: 0.8383, acc: 0.6950\n",
            "Iter 1200 - loss: 0.6008, acc: 0.8000\n",
            "Iter 1400 - loss: 0.6484, acc: 0.7550\n",
            "Iter 1600 - loss: 0.6374, acc: 0.7300\n",
            "Train set: Average loss: 0.6519, Accuracy: 1268/1692 (74.94%)\n",
            "Train Trial Accuracy: 74.94%\n",
            "\n",
            "Test set: Average loss: 0.8535, Accuracy: 274/423 (64.78%)\n",
            "Test Trial Accuracy: 64.78%\n",
            "\n",
            "Running epoch: 45\n",
            "Iter 200 - loss: 0.6532, acc: 0.7400\n",
            "Iter 400 - loss: 0.5721, acc: 0.8050\n",
            "Iter 600 - loss: 0.5683, acc: 0.8050\n",
            "Iter 800 - loss: 0.5219, acc: 0.8200\n",
            "Iter 1000 - loss: 0.7250, acc: 0.7350\n",
            "Iter 1200 - loss: 0.6317, acc: 0.7600\n",
            "Iter 1400 - loss: 0.6795, acc: 0.7450\n",
            "Iter 1600 - loss: 0.6047, acc: 0.7600\n",
            "Train set: Average loss: 0.6061, Accuracy: 1315/1692 (77.72%)\n",
            "Train Trial Accuracy: 77.72%\n",
            "\n",
            "Test set: Average loss: 0.8600, Accuracy: 278/423 (65.72%)\n",
            "Test Trial Accuracy: 65.72%\n",
            "\n",
            "Running epoch: 46\n",
            "Iter 200 - loss: 0.6268, acc: 0.7850\n",
            "Iter 400 - loss: 0.6889, acc: 0.7450\n",
            "Iter 600 - loss: 0.5565, acc: 0.8000\n",
            "Iter 800 - loss: 0.6122, acc: 0.7550\n",
            "Iter 1000 - loss: 0.7294, acc: 0.7200\n",
            "Iter 1200 - loss: 0.5950, acc: 0.7750\n",
            "Iter 1400 - loss: 0.6899, acc: 0.7450\n",
            "Iter 1600 - loss: 0.5996, acc: 0.7750\n",
            "Train set: Average loss: 0.6255, Accuracy: 1300/1692 (76.83%)\n",
            "Train Trial Accuracy: 76.83%\n",
            "\n",
            "Test set: Average loss: 0.8747, Accuracy: 269/423 (63.59%)\n",
            "Test Trial Accuracy: 63.59%\n",
            "\n",
            "Running epoch: 47\n",
            "Iter 200 - loss: 0.5800, acc: 0.7800\n",
            "Iter 400 - loss: 0.5951, acc: 0.7750\n",
            "Iter 600 - loss: 0.6004, acc: 0.7700\n",
            "Iter 800 - loss: 0.6197, acc: 0.7650\n",
            "Iter 1000 - loss: 0.7540, acc: 0.7000\n",
            "Iter 1200 - loss: 0.6283, acc: 0.7550\n",
            "Iter 1400 - loss: 0.6947, acc: 0.7400\n",
            "Iter 1600 - loss: 0.5570, acc: 0.8050\n",
            "Train set: Average loss: 0.6213, Accuracy: 1293/1692 (76.42%)\n",
            "Train Trial Accuracy: 76.42%\n",
            "\n",
            "Test set: Average loss: 0.9296, Accuracy: 269/423 (63.59%)\n",
            "Test Trial Accuracy: 63.59%\n",
            "\n",
            "Running epoch: 48\n",
            "Iter 200 - loss: 0.6622, acc: 0.7150\n",
            "Iter 400 - loss: 0.5398, acc: 0.7850\n",
            "Iter 600 - loss: 0.5445, acc: 0.8200\n",
            "Iter 800 - loss: 0.5752, acc: 0.7950\n",
            "Iter 1000 - loss: 0.7559, acc: 0.6900\n",
            "Iter 1200 - loss: 0.5691, acc: 0.7850\n",
            "Iter 1400 - loss: 0.6215, acc: 0.7700\n",
            "Iter 1600 - loss: 0.6167, acc: 0.7500\n",
            "Train set: Average loss: 0.5956, Accuracy: 1307/1692 (77.25%)\n",
            "Train Trial Accuracy: 77.25%\n",
            "\n",
            "Test set: Average loss: 0.8899, Accuracy: 275/423 (65.01%)\n",
            "Test Trial Accuracy: 65.01%\n",
            "\n",
            "Running epoch: 49\n",
            "Iter 200 - loss: 0.5625, acc: 0.7800\n",
            "Iter 400 - loss: 0.5940, acc: 0.7450\n",
            "Iter 600 - loss: 0.5618, acc: 0.7750\n",
            "Iter 800 - loss: 0.5566, acc: 0.7500\n",
            "Iter 1000 - loss: 0.6891, acc: 0.7600\n",
            "Iter 1200 - loss: 0.5686, acc: 0.8000\n",
            "Iter 1400 - loss: 0.6661, acc: 0.7400\n",
            "Iter 1600 - loss: 0.5885, acc: 0.7450\n",
            "Train set: Average loss: 0.5861, Accuracy: 1300/1692 (76.83%)\n",
            "Train Trial Accuracy: 76.83%\n",
            "\n",
            "Test set: Average loss: 0.8731, Accuracy: 274/423 (64.78%)\n",
            "Test Trial Accuracy: 64.78%\n",
            "\n",
            "Running epoch: 50\n",
            "Iter 200 - loss: 0.6206, acc: 0.7600\n",
            "Iter 400 - loss: 0.5491, acc: 0.7850\n",
            "Iter 600 - loss: 0.5422, acc: 0.7800\n",
            "Iter 800 - loss: 0.5795, acc: 0.7800\n",
            "Iter 1000 - loss: 0.6512, acc: 0.7550\n",
            "Iter 1200 - loss: 0.5549, acc: 0.7850\n",
            "Iter 1400 - loss: 0.6143, acc: 0.7750\n",
            "Iter 1600 - loss: 0.5691, acc: 0.7850\n",
            "Train set: Average loss: 0.5708, Accuracy: 1324/1692 (78.25%)\n",
            "Train Trial Accuracy: 78.25%\n",
            "\n",
            "Test set: Average loss: 0.8853, Accuracy: 272/423 (64.30%)\n",
            "Test Trial Accuracy: 64.30%\n",
            "\n",
            "Running epoch: 51\n",
            "Iter 200 - loss: 0.5856, acc: 0.7850\n",
            "Iter 400 - loss: 0.5483, acc: 0.8000\n",
            "Iter 600 - loss: 0.5298, acc: 0.8150\n",
            "Iter 800 - loss: 0.5419, acc: 0.8050\n",
            "Iter 1000 - loss: 0.6436, acc: 0.7700\n",
            "Iter 1200 - loss: 0.5728, acc: 0.7700\n",
            "Iter 1400 - loss: 0.5741, acc: 0.7650\n",
            "Iter 1600 - loss: 0.5934, acc: 0.7600\n",
            "Train set: Average loss: 0.5606, Accuracy: 1336/1692 (78.96%)\n",
            "Train Trial Accuracy: 78.96%\n",
            "\n",
            "Test set: Average loss: 0.9133, Accuracy: 263/423 (62.17%)\n",
            "Test Trial Accuracy: 62.17%\n",
            "\n",
            "Running epoch: 52\n",
            "Iter 200 - loss: 0.5156, acc: 0.8200\n",
            "Iter 400 - loss: 0.5025, acc: 0.8200\n",
            "Iter 600 - loss: 0.5181, acc: 0.8450\n",
            "Iter 800 - loss: 0.5723, acc: 0.7650\n",
            "Iter 1000 - loss: 0.6547, acc: 0.7550\n",
            "Iter 1200 - loss: 0.4922, acc: 0.8500\n",
            "Iter 1400 - loss: 0.5380, acc: 0.7950\n",
            "Iter 1600 - loss: 0.5407, acc: 0.8150\n",
            "Train set: Average loss: 0.5291, Accuracy: 1379/1692 (81.50%)\n",
            "Train Trial Accuracy: 81.50%\n",
            "\n",
            "Test set: Average loss: 0.8884, Accuracy: 269/423 (63.59%)\n",
            "Test Trial Accuracy: 63.59%\n",
            "\n",
            "Running epoch: 53\n",
            "Iter 200 - loss: 0.5303, acc: 0.7950\n",
            "Iter 400 - loss: 0.5059, acc: 0.7750\n",
            "Iter 600 - loss: 0.4961, acc: 0.8200\n",
            "Iter 800 - loss: 0.5822, acc: 0.7700\n",
            "Iter 1000 - loss: 0.6730, acc: 0.7600\n",
            "Iter 1200 - loss: 0.5377, acc: 0.8000\n",
            "Iter 1400 - loss: 0.5992, acc: 0.7500\n",
            "Iter 1600 - loss: 0.5440, acc: 0.7900\n",
            "Train set: Average loss: 0.5477, Accuracy: 1335/1692 (78.90%)\n",
            "Train Trial Accuracy: 78.90%\n",
            "\n",
            "Test set: Average loss: 0.8415, Accuracy: 281/423 (66.43%)\n",
            "Test Trial Accuracy: 66.43%\n",
            "\n",
            "Running epoch: 54\n",
            "Iter 200 - loss: 0.5744, acc: 0.7700\n",
            "Iter 400 - loss: 0.5964, acc: 0.7500\n",
            "Iter 600 - loss: 0.4626, acc: 0.8100\n",
            "Iter 800 - loss: 0.5579, acc: 0.7900\n",
            "Iter 1000 - loss: 0.6239, acc: 0.7800\n",
            "Iter 1200 - loss: 0.5755, acc: 0.7900\n",
            "Iter 1400 - loss: 0.5435, acc: 0.7950\n",
            "Iter 1600 - loss: 0.5095, acc: 0.8550\n",
            "Train set: Average loss: 0.5405, Accuracy: 1354/1692 (80.02%)\n",
            "Train Trial Accuracy: 80.02%\n",
            "\n",
            "Test set: Average loss: 0.8154, Accuracy: 285/423 (67.38%)\n",
            "Test Trial Accuracy: 67.38%\n",
            "\n",
            "Running epoch: 55\n",
            "Iter 200 - loss: 0.5636, acc: 0.7750\n",
            "Iter 400 - loss: 0.5455, acc: 0.7950\n",
            "Iter 600 - loss: 0.4941, acc: 0.8250\n",
            "Iter 800 - loss: 0.5162, acc: 0.8050\n",
            "Iter 1000 - loss: 0.7073, acc: 0.7400\n",
            "Iter 1200 - loss: 0.5301, acc: 0.8150\n",
            "Iter 1400 - loss: 0.5855, acc: 0.8200\n",
            "Iter 1600 - loss: 0.4805, acc: 0.8250\n",
            "Train set: Average loss: 0.5414, Accuracy: 1363/1692 (80.56%)\n",
            "Train Trial Accuracy: 80.56%\n",
            "\n",
            "Test set: Average loss: 0.8559, Accuracy: 273/423 (64.54%)\n",
            "Test Trial Accuracy: 64.54%\n",
            "\n",
            "Running epoch: 56\n",
            "Iter 200 - loss: 0.5755, acc: 0.7750\n",
            "Iter 400 - loss: 0.5345, acc: 0.7900\n",
            "Iter 600 - loss: 0.4613, acc: 0.8350\n",
            "Iter 800 - loss: 0.5310, acc: 0.8250\n",
            "Iter 1000 - loss: 0.5748, acc: 0.7800\n",
            "Iter 1200 - loss: 0.5159, acc: 0.8100\n",
            "Iter 1400 - loss: 0.6422, acc: 0.7600\n",
            "Iter 1600 - loss: 0.4876, acc: 0.8050\n",
            "Train set: Average loss: 0.5268, Accuracy: 1360/1692 (80.38%)\n",
            "Train Trial Accuracy: 80.38%\n",
            "\n",
            "Test set: Average loss: 0.8388, Accuracy: 278/423 (65.72%)\n",
            "Test Trial Accuracy: 65.72%\n",
            "\n",
            "Running epoch: 57\n",
            "Iter 200 - loss: 0.5436, acc: 0.7900\n",
            "Iter 400 - loss: 0.4594, acc: 0.8550\n",
            "Iter 600 - loss: 0.5647, acc: 0.7700\n",
            "Iter 800 - loss: 0.5011, acc: 0.7850\n",
            "Iter 1000 - loss: 0.5930, acc: 0.7900\n",
            "Iter 1200 - loss: 0.5075, acc: 0.7900\n",
            "Iter 1400 - loss: 0.5348, acc: 0.7950\n",
            "Iter 1600 - loss: 0.4685, acc: 0.8100\n",
            "Train set: Average loss: 0.5082, Accuracy: 1362/1692 (80.50%)\n",
            "Train Trial Accuracy: 80.50%\n",
            "\n",
            "Test set: Average loss: 0.8604, Accuracy: 280/423 (66.19%)\n",
            "Test Trial Accuracy: 66.19%\n",
            "\n",
            "Running epoch: 58\n",
            "Iter 200 - loss: 0.4703, acc: 0.8550\n",
            "Iter 400 - loss: 0.4857, acc: 0.8300\n",
            "Iter 600 - loss: 0.5083, acc: 0.8150\n",
            "Iter 800 - loss: 0.4314, acc: 0.8250\n",
            "Iter 1000 - loss: 0.6225, acc: 0.7600\n",
            "Iter 1200 - loss: 0.4394, acc: 0.8450\n",
            "Iter 1400 - loss: 0.6136, acc: 0.7550\n",
            "Iter 1600 - loss: 0.5090, acc: 0.8250\n",
            "Train set: Average loss: 0.4971, Accuracy: 1387/1692 (81.97%)\n",
            "Train Trial Accuracy: 81.97%\n",
            "\n",
            "Test set: Average loss: 0.8359, Accuracy: 292/423 (69.03%)\n",
            "Test Trial Accuracy: 69.03%\n",
            "\n",
            "Running epoch: 59\n",
            "Iter 200 - loss: 0.4766, acc: 0.8400\n",
            "Iter 400 - loss: 0.5280, acc: 0.8100\n",
            "Iter 600 - loss: 0.5158, acc: 0.7950\n",
            "Iter 800 - loss: 0.4788, acc: 0.8200\n",
            "Iter 1000 - loss: 0.5917, acc: 0.7950\n",
            "Iter 1200 - loss: 0.4611, acc: 0.8250\n",
            "Iter 1400 - loss: 0.4876, acc: 0.8050\n",
            "Iter 1600 - loss: 0.4651, acc: 0.8250\n",
            "Train set: Average loss: 0.4898, Accuracy: 1388/1692 (82.03%)\n",
            "Train Trial Accuracy: 82.03%\n",
            "\n",
            "Test set: Average loss: 0.8523, Accuracy: 282/423 (66.67%)\n",
            "Test Trial Accuracy: 66.67%\n",
            "\n",
            "Running epoch: 60\n",
            "Iter 200 - loss: 0.4176, acc: 0.8650\n",
            "Iter 400 - loss: 0.5038, acc: 0.8250\n",
            "Iter 600 - loss: 0.4230, acc: 0.8550\n",
            "Iter 800 - loss: 0.4531, acc: 0.8450\n",
            "Iter 1000 - loss: 0.5514, acc: 0.7900\n",
            "Iter 1200 - loss: 0.4753, acc: 0.8350\n",
            "Iter 1400 - loss: 0.4959, acc: 0.8150\n",
            "Iter 1600 - loss: 0.4539, acc: 0.8450\n",
            "Train set: Average loss: 0.4635, Accuracy: 1419/1692 (83.87%)\n",
            "Train Trial Accuracy: 83.87%\n",
            "\n",
            "Test set: Average loss: 0.8400, Accuracy: 284/423 (67.14%)\n",
            "Test Trial Accuracy: 67.14%\n",
            "\n",
            "Running epoch: 61\n",
            "Iter 200 - loss: 0.5117, acc: 0.8300\n",
            "Iter 400 - loss: 0.4819, acc: 0.8400\n",
            "Iter 600 - loss: 0.4207, acc: 0.8600\n",
            "Iter 800 - loss: 0.4493, acc: 0.8300\n",
            "Iter 1000 - loss: 0.5468, acc: 0.8150\n",
            "Iter 1200 - loss: 0.5025, acc: 0.8000\n",
            "Iter 1400 - loss: 0.4550, acc: 0.8450\n",
            "Iter 1600 - loss: 0.4119, acc: 0.8650\n",
            "Train set: Average loss: 0.4627, Accuracy: 1423/1692 (84.10%)\n",
            "Train Trial Accuracy: 84.10%\n",
            "\n",
            "Test set: Average loss: 0.8880, Accuracy: 278/423 (65.72%)\n",
            "Test Trial Accuracy: 65.72%\n",
            "\n",
            "Running epoch: 62\n",
            "Iter 200 - loss: 0.4743, acc: 0.8300\n",
            "Iter 400 - loss: 0.5083, acc: 0.8200\n",
            "Iter 600 - loss: 0.4580, acc: 0.8400\n",
            "Iter 800 - loss: 0.4580, acc: 0.8400\n",
            "Iter 1000 - loss: 0.5721, acc: 0.7550\n",
            "Iter 1200 - loss: 0.4207, acc: 0.8600\n",
            "Iter 1400 - loss: 0.5385, acc: 0.8200\n",
            "Iter 1600 - loss: 0.4807, acc: 0.8300\n",
            "Train set: Average loss: 0.4745, Accuracy: 1407/1692 (83.16%)\n",
            "Train Trial Accuracy: 83.16%\n",
            "\n",
            "Test set: Average loss: 0.9198, Accuracy: 274/423 (64.78%)\n",
            "Test Trial Accuracy: 64.78%\n",
            "\n",
            "Running epoch: 63\n",
            "Iter 200 - loss: 0.4657, acc: 0.8700\n",
            "Iter 400 - loss: 0.5687, acc: 0.8050\n",
            "Iter 600 - loss: 0.4238, acc: 0.8500\n",
            "Iter 800 - loss: 0.4449, acc: 0.8450\n",
            "Iter 1000 - loss: 0.5800, acc: 0.7700\n",
            "Iter 1200 - loss: 0.4659, acc: 0.8300\n",
            "Iter 1400 - loss: 0.4710, acc: 0.8200\n",
            "Iter 1600 - loss: 0.4255, acc: 0.8600\n",
            "Train set: Average loss: 0.4690, Accuracy: 1415/1692 (83.63%)\n",
            "Train Trial Accuracy: 83.63%\n",
            "\n",
            "Test set: Average loss: 0.9242, Accuracy: 270/423 (63.83%)\n",
            "Test Trial Accuracy: 63.83%\n",
            "\n",
            "Running epoch: 64\n",
            "Iter 200 - loss: 0.4876, acc: 0.8250\n",
            "Iter 400 - loss: 0.4731, acc: 0.8100\n",
            "Iter 600 - loss: 0.4489, acc: 0.8100\n",
            "Iter 800 - loss: 0.4521, acc: 0.8550\n",
            "Iter 1000 - loss: 0.4863, acc: 0.7950\n",
            "Iter 1200 - loss: 0.4467, acc: 0.8700\n",
            "Iter 1400 - loss: 0.4841, acc: 0.8350\n",
            "Iter 1600 - loss: 0.4279, acc: 0.8350\n",
            "Train set: Average loss: 0.4529, Accuracy: 1413/1692 (83.51%)\n",
            "Train Trial Accuracy: 83.51%\n",
            "\n",
            "Test set: Average loss: 0.9175, Accuracy: 269/423 (63.59%)\n",
            "Test Trial Accuracy: 63.59%\n",
            "\n",
            "Running epoch: 65\n",
            "Iter 200 - loss: 0.5107, acc: 0.8300\n",
            "Iter 400 - loss: 0.4134, acc: 0.8650\n",
            "Iter 600 - loss: 0.4108, acc: 0.8500\n",
            "Iter 800 - loss: 0.3881, acc: 0.8700\n",
            "Iter 1000 - loss: 0.5749, acc: 0.7700\n",
            "Iter 1200 - loss: 0.4495, acc: 0.8750\n",
            "Iter 1400 - loss: 0.5129, acc: 0.8000\n",
            "Iter 1600 - loss: 0.3830, acc: 0.8800\n",
            "Train set: Average loss: 0.4419, Accuracy: 1435/1692 (84.81%)\n",
            "Train Trial Accuracy: 84.81%\n",
            "\n",
            "Test set: Average loss: 0.9833, Accuracy: 274/423 (64.78%)\n",
            "Test Trial Accuracy: 64.78%\n",
            "\n",
            "Epoch    66: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Running epoch: 66\n",
            "Iter 200 - loss: 0.4831, acc: 0.8300\n",
            "Iter 400 - loss: 0.4322, acc: 0.8550\n",
            "Iter 600 - loss: 0.4104, acc: 0.8600\n",
            "Iter 800 - loss: 0.5109, acc: 0.8050\n",
            "Iter 1000 - loss: 0.5393, acc: 0.8250\n",
            "Iter 1200 - loss: 0.5581, acc: 0.7900\n",
            "Iter 1400 - loss: 0.4834, acc: 0.8150\n",
            "Iter 1600 - loss: 0.4403, acc: 0.8350\n",
            "Train set: Average loss: 0.4714, Accuracy: 1408/1692 (83.22%)\n",
            "Train Trial Accuracy: 83.22%\n",
            "\n",
            "Test set: Average loss: 0.8291, Accuracy: 281/423 (66.43%)\n",
            "Test Trial Accuracy: 66.43%\n",
            "\n",
            "Running epoch: 67\n",
            "Iter 200 - loss: 0.4541, acc: 0.8250\n",
            "Iter 400 - loss: 0.3941, acc: 0.8800\n",
            "Iter 600 - loss: 0.4289, acc: 0.8850\n",
            "Iter 800 - loss: 0.4040, acc: 0.8500\n",
            "Iter 1000 - loss: 0.5059, acc: 0.8100\n",
            "Iter 1200 - loss: 0.4088, acc: 0.8750\n",
            "Iter 1400 - loss: 0.4621, acc: 0.8350\n",
            "Iter 1600 - loss: 0.4402, acc: 0.8400\n",
            "Train set: Average loss: 0.4259, Accuracy: 1448/1692 (85.58%)\n",
            "Train Trial Accuracy: 85.58%\n",
            "\n",
            "Test set: Average loss: 0.8403, Accuracy: 277/423 (65.48%)\n",
            "Test Trial Accuracy: 65.48%\n",
            "\n",
            "Running epoch: 68\n",
            "Iter 200 - loss: 0.4925, acc: 0.8350\n",
            "Iter 400 - loss: 0.4810, acc: 0.8350\n",
            "Iter 600 - loss: 0.4014, acc: 0.8750\n",
            "Iter 800 - loss: 0.3848, acc: 0.8750\n",
            "Iter 1000 - loss: 0.4955, acc: 0.8050\n",
            "Iter 1200 - loss: 0.4863, acc: 0.8500\n",
            "Iter 1400 - loss: 0.4489, acc: 0.8450\n",
            "Iter 1600 - loss: 0.3733, acc: 0.8800\n",
            "Train set: Average loss: 0.4327, Accuracy: 1449/1692 (85.64%)\n",
            "Train Trial Accuracy: 85.64%\n",
            "\n",
            "Test set: Average loss: 0.8208, Accuracy: 281/423 (66.43%)\n",
            "Test Trial Accuracy: 66.43%\n",
            "\n",
            "Running epoch: 69\n",
            "Iter 200 - loss: 0.4791, acc: 0.8450\n",
            "Iter 400 - loss: 0.4815, acc: 0.8300\n",
            "Iter 600 - loss: 0.3868, acc: 0.8800\n",
            "Iter 800 - loss: 0.3995, acc: 0.8650\n",
            "Iter 1000 - loss: 0.4536, acc: 0.8350\n",
            "Iter 1200 - loss: 0.4073, acc: 0.8650\n",
            "Iter 1400 - loss: 0.4470, acc: 0.8400\n",
            "Iter 1600 - loss: 0.3908, acc: 0.8700\n",
            "Train set: Average loss: 0.4208, Accuracy: 1452/1692 (85.82%)\n",
            "Train Trial Accuracy: 85.82%\n",
            "\n",
            "Test set: Average loss: 0.8399, Accuracy: 282/423 (66.67%)\n",
            "Test Trial Accuracy: 66.67%\n",
            "\n",
            "Stopping early due to stagnation of test loss\n",
            "\n",
            "Best test loss was 0.8153724895865078\n",
            "Best test accuracy was 69.03073286052009\n",
            "\n",
            "Save model and training history? (y/n): y\n",
            "Enter your name: edward\n",
            "Enter unique model identifier (to be used as part of filename): cnn_schirr_69\n",
            "Saved CNN_Schirr_cnn_schirr_69 model from epoch 54 with test loss 0.8153724895865078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcecH9VOCcmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}